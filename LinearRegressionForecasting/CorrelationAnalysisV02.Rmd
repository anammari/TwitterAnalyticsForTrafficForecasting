---
title: "Cross-Correlation Analysis of Sensor Readings & Twitter-derived Features"
author: "Optimum Project"
date: "26 May 2016"
output: html_document
---

```{r echo=FALSE, message=F, warning=F, comment=NA}
library(knitr)
require(futile.logger)
require(xts)
require(rmongodb)
require(ggplot2)
require(graphics)
require(astsa)
require(zoo)
require(fpp)
require(forecast)
require(scales)
dummy <- Sys.setenv(TZ='GMT')
# dummy <- Sys.setlocale("LC_ALL", "English_United Kingdom.1252")

#util functions
Find_Max_CCF<- function(a,b)
{
  d <- ccf(a, b, plot = FALSE)
  cor = d$acf[,,1]
  lag = d$lag[,,1]
  res = data.frame(cor,lag)
  res_max = res[which.max(res$cor),]
  return(res_max)
}

lm_eqn <- function(df){
  m <- lm(y ~ x, df);
  eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
                   list(a = format(coef(m)[1], digits = 2), 
                        b = format(coef(m)[2], digits = 2), 
                        r2 = format(summary(m)$r.squared, digits = 3)))
  as.character(as.expression(eq));                 
}

trim <- function (x) gsub("^\\s+|\\s+$", "", x)

list.dirs <- function(path, pattern=NULL, all.dirs=FALSE, full.names=TRUE, ignore.case=FALSE) {
  # use full.names=TRUE to pass to file.info
  all <- list.files(path, pattern, all.dirs,
                    full.names=TRUE, recursive=FALSE, ignore.case=TRUE)
  # determine whether to return full names or just dir names
  if(isTRUE(full.names))
    return(all)
  else
    return(basename(all))
}

#set working directory
setwd("C:\\Users\\Ahmad\\Dropbox\\Work\\WolverhamptonUni\\docs\\D3.3\\Analysis\\TwitterNTISCorrelation")

#sensor data directory
dataDir <- "C:\\Users\\Ahmad\\Dropbox\\Work\\WolverhamptonUni\\docs\\D3.3\\Analysis\\TwitterNTISCorrelation\\NTISData\\"

```

##Objective 

This report aims to investigate the identification of relationship between two types of time series data sets. The first time series type is the sets of average speed and flow traffic measurements as streamed from traffic loop sensors (NTIS) from specific pre-selected loop sensors on specific highways in the city of Birmingham. The second time series type is the sets of hourly-based frequency aggregations of semantics derived from the social media site Twitter.     

In the relationship between two sets of time series data (y(t) and x(t)), the series y(t) may be related to past lags of the x-series. The sample [cross-correlation function (CCF)](https://onlinecourses.science.psu.edu/stat510/node/74) is helpful for identifying lags of the x-variable that might be useful predictors of y(t).

The ground definition of the analysis is that the **sample CCF** is defined as the **set of sample correlations between x(t+h) and y(t) for h = 0, ±1, ±2, ±3, and so on**.  A negative value for the lag (h) is a correlation between the x-variable at a time before t and the y-variable at time t. For instance, in the case of h = -2, The CCF value would give the correlation between x(t-2) and y(t).

A basic assumption in our analysis is that we will examine the Twitter-derived time series data to be a leading x-variable(s), and the sensor time series data to be the y-variable. This is because we aim to use values of the Twitter-derived features (explanatory x-variable(s)) to predict future values of the sensor data (response y-variable).

##Data Sets Info 

###Sensor Data Sets

```{r echo=FALSE, message=F, warning=F, comment=NA}
#Load the sensor data sets
sensors_full_filenames <- list.dirs(dataDir,full.names=TRUE)
sensors_filenames <- list.dirs(dataDir,full.names=FALSE)
sensorDfs <- list()

sumtabDf <- data.frame(
  highway = character(0),
  mId = character(0),
  mType = character(0),
  startDt = as.POSIXct(character()),
  endDt = as.POSIXct(character()),
  numMeasurement = numeric(0),
  stringsAsFactors = F
)
for (i in 1:length(sensors_filenames)) {
  namePart <- unlist(strsplit(sensors_filenames[i], "\\."))[1]
  highway <- unlist(strsplit(namePart, "_"))[2]
  mId <- unlist(strsplit(namePart, "_"))[1]
  mType <- unlist(strsplit(namePart, "_"))[3]
  data <- read.table(file = sensors_full_filenames[i], sep = " ", header = FALSE, fileEncoding = "utf8")
  colnames(data) <- c("datetime","V")
  data$datetime <- strptime(data$datetime, format = "%Y-%m-%dT%H:%M:%S") - 3600
  attr(data$datetime, "tzone") <- "GMT"
  data <- data[with(data, order(datetime)), ]
  data <- data[with(data, data$datetime >= "2016-04-13 00:00:00"),]
  end_records <- nrow(data[with(data, data$datetime >= "2016-05-05 00:00:00"),])
  if (end_records > 0) {
    data <- data[with(data, data$datetime < "2016-06-22 00:00:00"),]
  } else {
    data <- data[with(data, data$datetime < "2016-05-04 00:00:00"),]
  }
  startDt <- min(data$datetime)
  endDt <- max(data$datetime)
  numMeasurement <- nrow(data)
  
  sumtabDf <- rbind(sumtabDf, data.frame(
                                highway = highway,
                                mId = mId,
                                mType = mType,
                                startDt = strftime(startDt, format = "%Y-%m-%dT%H:%M:%S"),
                                endDt = strftime(endDt, format = "%Y-%m-%dT%H:%M:%S"),
                                numMeasurement = numMeasurement,
                                stringsAsFactors = F ))
  sensorDfs[[i]] <- data
}
colnames(sumtabDf) <- c("Highway","Measurement Id","Measurement Type","Start DateTime","End DateTime","No of measurements")
kable(sumtabDf, format = "markdown", caption = "Sensor Data Sets")

```

**Note:** The date/times are in the GMT time zone. 

###Twitter Data Sets

The Twitter-derived features to be correlated with the traffic sensor data will consist of the following datasets:

- Hourly aggregations of the frequency of the mentioned Highways in the tweet contents. The aggregated mentions of a specific highway name (e.g. M42, M6, M54) presented in each sensor dataset will be correlated with the correspondent sensor dataset for that matching highway name. 

- Hourly aggregations of the frequency of **highway-level** traffic concepts based on a traffic gazetteer. The aggregated mentions of a specific highway name (e.g. M42, M6, M54) presented in each sensor dataset will be correlated with the correspondent sensor dataset for that matching highway name.


##Analysis Output

The output of each correlation experiment will consist of:

- Timeline plots of the correlated datasets (hourly mean aggregations (sensor data) & mention count aggregations (Twitter data)) after applying log transformation on the two data sets

- Cross-correlation (CCF) plots of the correlated datasets (hourly mean aggregations (sensor data) & mention count aggregations (Twitter data))

-  Scatter plots y(t) versus x(t+h) for lags (h) starting from 0 back to the lag that produces the maximum CCF. Correlation values will be given on each plot.

- Comparison on Time Series Regression Models with and without lagged Twitter features

##Correlation Analysis: Sensor Data with Twitter Highway Mentions

```{r echo=FALSE, message=F, warning=F, comment=NA, fig.width=16, fig.height=12}

mergedDfs <- list()
for (i in 1: length(sensorDfs)) {
  print(kable(sumtabDf[i,c(2,1,3)], format = "markdown", caption = "Sensor Data Set"))
  
  #aggregate sensor data
  sensorDf <- sensorDfs[[i]]
  highway <- sumtabDf$Highway[i]
  mType <- sumtabDf[i,3]
  sensor.ts <- xts(sensorDf[,-1], order.by=as.POSIXct(sensorDf$datetime))
  aggs.ts <- period.apply(sensor.ts, endpoints(sensor.ts, "hours"), function(x) mean(x))
  
  aggs.tsAvgAggs = data.frame(
    datetime = as.POSIXct(index(aggs.ts)) - 3540,
    value = aggs.ts[,c(1)],
    stringsAsFactors = F
  )
  aggs.tsAvgAggs$datetime <- aggs.tsAvgAggs$datetime
  row.names(aggs.tsAvgAggs) <- NULL
  
  minDate <- min(aggs.tsAvgAggs$datetime)
  maxDate <- max(aggs.tsAvgAggs$datetime)
  
  #retrieve the aggs of the traffic mentions for the same road and sum them into 1 data frame
  #create dataframes to store the data
  accidentTwitterAggs = data.frame(
    gmt_date = as.POSIXct(character()),
    value = numeric(0),
    stringsAsFactors = F
  )
  congestionTwitterAggs = data.frame(
    gmt_date = as.POSIXct(character()),
    value = numeric(0),
    stringsAsFactors = F
  )
  delayTwitterAggs = data.frame(
    gmt_date = as.POSIXct(character()),
    value = numeric(0),
    stringsAsFactors = F
  )
  #connect to mongoDB
  mongo <- mongo.create(host = "XXX:XXX")
  #accident data
  if (mongo.is.connected(mongo) == TRUE) {
    queryList <- mongo.bson.from.list(
      list('gmt_date' = list(
        "$gte" = minDate,
        "$lte" = maxDate
      ),
      "road" = highway,
      "traffic_concept" = "ACCIDENT"))
    tweets_count <- mongo.count(mongo,"Twitter.uk_accounts_traffic_hr_agg_roads",query=queryList)
    if (tweets_count > 0) {
      flog.info("Total records to be fetched for %s about accident from %s to %s: %s", highway, minDate, maxDate, tweets_count)
      result <- mongo.find(mongo, "Twitter.uk_accounts_traffic_hr_agg_roads", query = queryList)
      while (mongo.cursor.next(result)) {
        tdDt <- .POSIXct(character())
        l <- list(mongo.bson.to.list(mongo.cursor.value(result)))
        tdDt <- l[[1]]$gmt_date
        value <- l[[1]]$value
        
        accidentTwitterAggs <-rbind(accidentTwitterAggs,
                                   data.frame(gmt_date = tdDt,
                                              value = value,
                                              stringsAsFactors = FALSE)
        )
      }
      attr(accidentTwitterAggs$gmt_date, "tzone") <- "GMT"
      accidentTwitterAggs <- accidentTwitterAggs[with(accidentTwitterAggs, order(gmt_date)), ]
    } else {
      flog.info("No records found from twitter for Accident in highway %s from %s to %s", highway, minDate, maxDate)
      next
    }
  }
  #congestion data
  if (mongo.is.connected(mongo) == TRUE) {
    queryList <- mongo.bson.from.list(
      list('gmt_date' = list(
        "$gte" = minDate,
        "$lte" = maxDate
      ),
      "road" = highway,
      "traffic_concept" = "CONGESTION"))
    tweets_count <- mongo.count(mongo,"Twitter.uk_accounts_traffic_hr_agg_roads",query=queryList)
    if (tweets_count > 0) {
      flog.info("Total records to be fetched for %s about congestion from %s to %s: %s", highway, minDate, maxDate, tweets_count)
      result <- mongo.find(mongo, "Twitter.uk_accounts_traffic_hr_agg_roads", query = queryList)
      while (mongo.cursor.next(result)) {
        tdDt <- .POSIXct(character())
        l <- list(mongo.bson.to.list(mongo.cursor.value(result)))
        tdDt <- l[[1]]$gmt_date
        value <- l[[1]]$value
        
        congestionTwitterAggs <-rbind(congestionTwitterAggs,
                                   data.frame(gmt_date = tdDt,
                                              value = value,
                                              stringsAsFactors = FALSE)
        )
      }
      attr(congestionTwitterAggs$gmt_date, "tzone") <- "GMT"
      congestionTwitterAggs <- congestionTwitterAggs[with(congestionTwitterAggs, order(gmt_date)), ]
    } else {
      flog.info("No records found from twitter for Congestion in highway %s from %s to %s", highway, minDate, maxDate)
      next
    }
  }
  #delay data
  if (mongo.is.connected(mongo) == TRUE) {
    queryList <- mongo.bson.from.list(
      list('gmt_date' = list(
        "$gte" = minDate,
        "$lte" = maxDate
      ),
      "road" = highway,
      "traffic_concept" = "DELAY"))
    tweets_count <- mongo.count(mongo,"Twitter.uk_accounts_traffic_hr_agg_roads",query=queryList)
    if (tweets_count > 0) {
      flog.info("Total records to be fetched for %s about delay from %s to %s: %s", highway, minDate, maxDate, tweets_count)
      result <- mongo.find(mongo, "Twitter.uk_accounts_traffic_hr_agg_roads", query = queryList)
      while (mongo.cursor.next(result)) {
        tdDt <- .POSIXct(character())
        l <- list(mongo.bson.to.list(mongo.cursor.value(result)))
        tdDt <- l[[1]]$gmt_date
        value <- l[[1]]$value
        
        delayTwitterAggs <-rbind(delayTwitterAggs,
                                   data.frame(gmt_date = tdDt,
                                              value = value,
                                              stringsAsFactors = FALSE)
        )
      }
      attr(delayTwitterAggs$gmt_date, "tzone") <- "GMT"
      delayTwitterAggs <- delayTwitterAggs[with(delayTwitterAggs, order(gmt_date)), ]
    } else {
      flog.info("No records found from twitter for Delay in highway %s from %s to %s", highway, minDate, maxDate)
      next
    }
  }
  #sum the incident values for same hours and fill the unavailable twitter dataset hours with zeros
  highwayTwitterAggsFull <- data.frame(
    gmt_date = as.POSIXct(character()),
    value = numeric(0),
    stringsAsFactors = F
  )
  
  minDateTwit <- minDate
  maxDateTwit <- maxDate
  while (minDateTwit <= maxDateTwit) {
    accidentValue <- accidentTwitterAggs[accidentTwitterAggs$gmt_date == minDateTwit,c("value")]
    if (length(accidentValue) == 0) {
      accidentValue <- 0
    }
    congestionValue <- congestionTwitterAggs[congestionTwitterAggs$gmt_date == minDateTwit,c("value")]
    if (length(congestionValue) == 0) {
      congestionValue <- 0
    }
    delayValue <- delayTwitterAggs[delayTwitterAggs$gmt_date == minDateTwit,c("value")]
    if (length(delayValue) == 0) {
      delayValue <- 0
    }
    value <- accidentValue + congestionValue + delayValue
    if (length(value) > 0) {
      highwayTwitterAggsFull <-rbind(highwayTwitterAggsFull,
                                 data.frame(gmt_date = minDateTwit,
                                            value = value,
                                            stringsAsFactors = FALSE)
      )
    } else {
      #flog.info("No record found in gmt_date: %s", strftime(minDateTwit,"%Y-%m-%d %H:%M:%S",tz="GMT"))
      highwayTwitterAggsFull <-rbind(highwayTwitterAggsFull,
                                     data.frame(gmt_date = minDateTwit,
                                                value = 0,
                                                stringsAsFactors = FALSE)
      )
    }
    minDateTwit <- minDateTwit + 3600
  }
  attr(highwayTwitterAggsFull$gmt_date, "tzone") <- "GMT"
  # highwayTwitterAggsFull$value <- na.approx(highwayTwitterAggsFull$value, x = index(highwayTwitterAggsFull$gmt_date), na.rm = TRUE, maxgap = Inf)
  #merge the 2 datasets
  mergedDt <- merge(aggs.tsAvgAggs, highwayTwitterAggsFull, by.x = "datetime", by.y = "gmt_date")
  mergedDfs[[i]] <- mergedDt
}

#Analysis. Do it individually
#*****************************

mergedDt <- mergedDfs[[12]]
#1.Original values of data:
#compute the cross-correlation plots and find the lag that gives
# the optimal correlation
ccf(mergedDt$value.y, mergedDt$value.x, ylab = "cross-correlation")
ccfvalues <- ccf(mergedDt$value.y, mergedDt$value.x, 
			ylab = "cross-correlation")
ccfvalues
#kable(Find_Max_CCF(mergedDt$value.y, mergedDt$value.x))
#scatter plots
lag2.plot (mergedDt$value.y, mergedDt$value.x, 
	abs(Find_Max_CCF(mergedDt$value.y, mergedDt$value.x)[1,2]))

#Linear Regression with and without Twitter predictors
colnames(mergedDt) <- c("datetime","sensorV","twitterV")
sensTs <- ts(mergedDt$sensorV)
twitTs <- ts(mergedDt$twitterV)
alldata <- ts.intersect(sensTs,sensTslag1=lag(sensTs,-1), sensTslag2=lag(sensTs,-2), 
                     twitTslag7=lag(twitTs,-7),
                     twitTslag8=lag(twitTs,-8), 
                     twitTslag9=lag(twitTs,-9) 
                      )
tryit = lm(sensTs~twitTslag7+twitTslag8+twitTslag9,data = alldata)
lmOut(tryit, file="tryit.csv")
tryit2 = lm(sensTs~sensTslag1+sensTslag2+twitTslag7+twitTslag8+twitTslag9,data = alldata)
lmOut(tryit2, file="tryit2.csv")
tryit3 = lm(sensTs~sensTslag1+sensTslag2, data = alldata)
lmOut(tryit3, file="tryit3.csv")

#2.With log transformation
#first remove all rows having 0 readings
mergedDtNoZero <- mergedDt[mergedDt$value.y > 0 & mergedDt$value.x > 0, ]
scaledDF <- log(mergedDtNoZero[,c(2,3)])
scaledDF <- as.data.frame(cbind(mergedDtNoZero[,c(1)],scaledDF))
colnames(scaledDF) <- c("datetime","sensorV","twitterV")
scaledDF <- as.data.frame(scaledDF)
#compute the cross-correlation plots and find the lag that gives the optimal correlation
ccf(scaledDF$twitterV, scaledDF$sensorV, ylab = "cross-correlation")
ccfvalues <- ccf(scaledDF$twitterV, scaledDF$sensorV, ylab = "cross-correlation")
ccfvalues
#kable(Find_Max_CCF(scaledDF$twitterV, scaledDF$sensorV))

#Linear Regression with and without Twitter predictors
sensTs <- ts(scaledDF$sensorV)
twitTs <- ts(scaledDF$twitterV)
alldata <- ts.intersect(sensTs,sensTslag1=lag(sensTs,-1),sensTslag2=lag(sensTs,-2),
                     twitTslag1=lag(twitTs,-1),
                     twitTslag21=lag(twitTs,-21),
                     twitTslag22=lag(twitTs,-22),
                     twitTslag23=lag(twitTs,-23), 
                     twitTslag8=lag(twitTs,-8),
                     twitTslag9=lag(twitTs,-9), 
                     twitTslag10=lag(twitTs,-10), 
                     twitTslag11=lag(twitTs,-11), 
                     twitTslag12=lag(twitTs,-12),
                     twitTslag19=lag(twitTs,-19),
                     twitTslag20=lag(twitTs,-20),
                     twitTslag21=lag(twitTs,-21),
                     twitTslag22=lag(twitTs,-22),
                     twitTslag23=lag(twitTs,-23)
                    )
tryit = lm(sensTs~twitTslag1+twitTslag21+twitTslag22+twitTslag23, data = alldata)
lmOut(tryit, file="tryit.csv")
tryit2 = lm(sensTs~sensTslag1+sensTslag2+twitTslag1+twitTslag21+twitTslag22+twitTslag23, data = alldata)
lmOut(tryit2, file="tryit2.csv")
tryit3 = lm(sensTs~sensTslag1+sensTslag2, data = alldata)
lmOut(tryit3, file="tryit3.csv")

#scatter plots
lag2.plot (mergedDt$value.y, mergedDt$value.x, abs(Find_Max_CCF(mergedDt$value.y, mergedDt$value.x)[1,2]))
#plot histograms
hist(scaledDF$sensorV)
hist(scaledDF$twitterV)
#plot timelines
minDate <- min(scaledDF$datetime)
maxDate <- max(scaledDF$datetime)
limits = c(minDate,maxDate)
ggplot(scaledDF, aes(datetime)) + 
geom_line(aes(y = sensorV, colour = "sensorV")) +
geom_line(aes(y = twitterV, colour = "twitterV")) +
scale_x_datetime( date_breaks=("6 hour"),
                  date_labels="%m-%d:%H",
                  limits=limits) + 
xlab("") + 
ylab("Sensor Data Vs. Twitter Data") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      axis.text.y = element_text(size = 10),
      legend.text = element_text(size = 12))


#plot scatter plots of twitter vs sensor
mergedDtLm <- mergedDt[,c(2,3)]
colnames(mergedDtLm) <- c("y","x")
p <- ggplot(mergedDtLm, aes(x=x, y=y)) + 
    geom_smooth(method = "lm", se=FALSE, color="black", formula = y ~ x) +
    geom_point(shape=20, size = 5) +       
    xlab("Hourly mentions of Twitter events") +
    ylab("Hourly speed means") +
    #scale_y_discrete(breaks = seq(min(pol_dist$volume), max(pol_dist$volume), by = 100), labels = comma) + 
    ggtitle("Twitter Traffic Events VS Sensor Speed Readings") + 
    theme(plot.title = element_text(color="#666666", face="bold", size=14),
          axis.text.x=element_text(size=14), 
          axis.text.y=element_text(size=14),
          axis.title.x=element_text(size=14,face="bold"),
          axis.title.y=element_text(size=14,face="bold")) +
    geom_text(x = 5 * mean(mergedDtLm$x), y = 0.6 * mean(mergedDtLm$y), 
              label = lm_eqn(mergedDtLm), size=8, colour = "black", parse = TRUE)
print(p)
```
